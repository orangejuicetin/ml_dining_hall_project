{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====================================\n",
      "Visualization of MLP weights on MNIST\n",
      "=====================================\n",
      "\n",
      "Sometimes looking at the learned coefficients of a neural network can provide\n",
      "insight into the learning behavior. For example if weights look unstructured,\n",
      "maybe some were not used at all, or if very large coefficients exist, maybe\n",
      "regularization was too low or the learning rate too high.\n",
      "\n",
      "This example shows how to plot some of the first layer weights in a\n",
      "MLPClassifier trained on the MNIST dataset.\n",
      "\n",
      "The input data consists of 28x28 pixel handwritten digits, leading to 784\n",
      "features in the dataset. Therefore the first layer weight matrix have the shape\n",
      "(784, hidden_layer_sizes[0]).  We can therefore visualize a single column of\n",
      "the weight matrix as a 28x28 pixel image.\n",
      "\n",
      "To make the example run faster, we use very few hidden units, and train only\n",
      "for a very short time. Training longer would result in weights with a much\n",
      "smoother spatial appearance.\n",
      "\n",
      "Iteration 1, loss = 0.32009978\n",
      "Iteration 2, loss = 0.15347534\n",
      "Iteration 3, loss = 0.11544755\n",
      "Iteration 4, loss = 0.09279764\n",
      "Iteration 5, loss = 0.07889367\n",
      "Iteration 6, loss = 0.07170497\n",
      "Iteration 7, loss = 0.06282111\n",
      "Iteration 8, loss = 0.05530788\n",
      "Iteration 9, loss = 0.04960484\n",
      "Iteration 10, loss = 0.04645355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.986800\n",
      "Test set score: 0.970000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "=====================================\n",
    "Visualization of MLP weights on MNIST\n",
    "=====================================\n",
    "\n",
    "Sometimes looking at the learned coefficients of a neural network can provide\n",
    "insight into the learning behavior. For example if weights look unstructured,\n",
    "maybe some were not used at all, or if very large coefficients exist, maybe\n",
    "regularization was too low or the learning rate too high.\n",
    "\n",
    "This example shows how to plot some of the first layer weights in a\n",
    "MLPClassifier trained on the MNIST dataset.\n",
    "\n",
    "The input data consists of 28x28 pixel handwritten digits, leading to 784\n",
    "features in the dataset. Therefore the first layer weight matrix have the shape\n",
    "(784, hidden_layer_sizes[0]).  We can therefore visualize a single column of\n",
    "the weight matrix as a 28x28 pixel image.\n",
    "\n",
    "To make the example run faster, we use very few hidden units, and train only\n",
    "for a very short time. Training longer would result in weights with a much\n",
    "smoother spatial appearance.\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "# Load data from https://www.openml.org/d/554\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "X = X / 255.\n",
    "\n",
    "# rescale the data, use the traditional train/test split\n",
    "X_train, X_test = X[:60000], X[60000:]\n",
    "y_train, y_test = y[:60000], y[60000:]\n",
    "\n",
    "# mlp = MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=400, alpha=1e-4,\n",
    "#                     solver='sgd', verbose=10, tol=1e-4, random_state=1)\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(50,), max_iter=10, alpha=1e-4,\n",
    "                    solver='sgd', verbose=10, tol=1e-4, random_state=1,\n",
    "                    learning_rate_init=.1)\n",
    "\n",
    "mlp.fit(X_train, y_train)\n",
    "print(\"Training set score: %f\" % mlp.score(X_train, y_train))\n",
    "print(\"Test set score: %f\" % mlp.score(X_test, y_test))\n",
    "\n",
    "fig, axes = plt.subplots(4, 4)\n",
    "# use global min / max to ensure all weights are shown on the same scale\n",
    "vmin, vmax = mlp.coefs_[0].min(), mlp.coefs_[0].max()\n",
    "for coef, ax in zip(mlp.coefs_[0].T, axes.ravel()):\n",
    "    ax.matshow(coef.reshape(28, 28), cmap=plt.cm.gray, vmin=.5 * vmin,\n",
    "               vmax=.5 * vmax)\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
